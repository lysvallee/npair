# Ethical 3D Content Generation ğŸŒŸ

## Overview
A proof-of-concept web application demonstrating responsible AI-powered 3D object generation, built with a commitment to copyright respect and ethical AI practices.

## Key Features
- ğŸ¤– Utilizes TripoSR model for 3D object generation
- ğŸ–¼ï¸ Powered by Pixabay's royalty-free image database
- ğŸ›¡ï¸ 100% legal AI workflow
- ğŸš€ Built with FastAPI and Docker for robust deployment

## The Ethical AI Approach
This project showcases how AI can generate 3D content while maintaining strict adherence to copyright principles:
- Model trained on CC-BY licensed Objaverse dataset
- Input images sourced from Pixabay's comprehensive royalty-free library

## Tech Stack
- FastAPI
- Docker
- TripoSR Model
- Pixabay API

The application consists of three main components:
1. **Data API (data_api):** Serves images and tabular data (color palettes, PBR materials) from Apache Cassandra and PostgreSQL databases.
2. **Model API (model_api):** Runs the TripoSR model to generate 3D objects from images received through the API.
3. **User API (user_api):** Receives user input, interacts with the data and model APIs, and provides the generated 3D object to the frontend.

It can be deployed in production and testing environments using Docker Compose. Monitoring for the user API is also integrated with Grafana.

## Setting Up the Project

**Prerequisites:**

* Docker: [https://docs.docker.com/engine/install/](https://docs.docker.com/engine/install/)
* Docker Compose: [https://docs.docker.com/compose/install/](https://docs.docker.com/compose/install/)
* Python 3.x

**1. Clone the Repository:**

```bash
git clone https://github.com/lysvallee/npair.git
```


**2. Build Docker Images (Optional):**

You can build the Docker images for each component manually:

```bash
cd data/api
docker build -t data_api .

cd ../model/api
docker build -t model_api .

# ... Repeat for other services (user_api, tests, etc.)
```

**3. Run the Application (Development or Production Mode):**

Navigate to the root directory:

```bash
cd npair
```

Use Docker Compose to build the images (if necessary) and start the application in your desired mode:

```bash
docker-compose -f docker-compose.dev.yml up --build
or
docker-compose -f docker-compose.prod.yml up --build
or
docker-compose -f docker-compose.track.yml up --build
```

This will build the required images and bring up the concerned services. You can access the APIs at the following default ports:

* Data API: http://localhost:8000 (modify port if needed in the Docker Compose yml)
* Model API: http://localhost:8001
* User API: http://localhost:8002


## API Usage

**1. Data API:**

The Data API provides endpoints for accessing images and tabular data. Refer to `data/api/data_api.py` for detailed documentation and available endpoints.

**2. Model API:**

The Model API takes an image as input and generates a 3D object using the TripoSR model. Documentation and endpoint details are available in `model/api/model_api.py`.

**3. User API:**

The User API serves as the primary entry point for users. It interacts with the Data and Model APIs to provide a combined functionality:

* Users can choose an image through a POST request to a specific endpoint.
* The service then calls the Model API with the image path and receives the generated 3D object.
* It retrieves additional data (color palettes, materials) from the Data API that allow further enhancement.
* It returns the final 3D object to the user.

Please refer to `user/api/user_api.py` for specific endpoint details and usage instructions.

**Note:** This is a general overview. Specific API endpoints, request/response formats, and authentication mechanisms might require further exploration within the relevant API code files.

## Monitoring

The user API is integrated with Grafana for monitoring. Please refer to the `user/monitoring` folder for configuration details.

## Model Tracking

To run experiments, set the hyperparameters with tracking_api.py, use docker-compose.track.yml, then start the process with:
```bash
curl -X POST http://localhost:5000/run_experiments
```
Logs are stored in logs/model_api_track.log.
The resulting metrics are also available in the model_metrics table of the database.


Full file structure:

```
â”œâ”€â”€ data
â”‚Â Â  â”œâ”€â”€ api
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ clear_databases.py
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ data_api.py
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ data_collection.md
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ Dockerfile
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ extract_brand_palettes.py
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ extract_images.py
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ extract_materials.py
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ extract_movie_palettes.py
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ extract_show_palettes.py
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ initialize.sh
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ initial_setup.py
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ models.py
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ pip_cache
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ requirements.txt
â”‚Â Â  â”‚Â Â  â””â”€â”€ services.py
â”‚Â Â  â””â”€â”€ storage
â”‚Â Â      â”œâ”€â”€ cdb
â”‚Â Â      â”œâ”€â”€ db
â”‚Â Â      â”œâ”€â”€ images
â”‚Â Â      â”œâ”€â”€ materials
â”‚Â Â      â”œâ”€â”€ objects
â”‚Â Â      â”œâ”€â”€ palettes
â”‚Â Â      â”œâ”€â”€ renders
â”‚Â Â      â””â”€â”€ test_data
â”œâ”€â”€ docker-compose.dev.yml
â”œâ”€â”€ docker-compose.prod.yml
â”œâ”€â”€ docker-compose.track.yml
â”œâ”€â”€ grafana
â”‚Â Â  â”œâ”€â”€ data
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ csv
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ grafana.db
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ pdf
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ plugins
â”‚Â Â  â”‚Â Â  â””â”€â”€ png
â”‚Â Â  â”œâ”€â”€ monitoring.md
â”‚Â Â  â””â”€â”€ provisioning
â”‚Â Â      â”œâ”€â”€ alerting
â”‚Â Â      â”œâ”€â”€ dashboards
â”‚Â Â      â”œâ”€â”€ datasources
â”‚Â Â      â””â”€â”€ plugins
â”œâ”€â”€ logs
â”‚Â Â  â”œâ”€â”€ clear_databases.log
â”‚Â Â  â”œâ”€â”€ create_movies_db.log
â”‚Â Â  â”œâ”€â”€ create_shows_db.log
â”‚Â Â  â”œâ”€â”€ data_api.log
â”‚Â Â  â”œâ”€â”€ extract_movie_palettes.log
â”‚Â Â  â”œâ”€â”€ model_api.log
â”‚Â Â  â”œâ”€â”€ model_api_track.log
â”‚Â Â  â”œâ”€â”€ tracking_api.log
â”‚Â Â  â””â”€â”€ user_api.log
â”œâ”€â”€ model
â”‚Â Â  â”œâ”€â”€ api
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ Dockerfile.cpu
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ Dockerfile.gpu
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ logs
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ model_api.py
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ model_api_track.py
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ model_configuration.md
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ model_integration.md
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ models.py
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ pip_cache
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ __pycache__
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ pytest.ini
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ requirements.txt
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ services.py
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ test_model_api.md
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ test_model_api.py
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ triposr
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ unit_tests.md
â”‚Â Â  â”‚Â Â  â””â”€â”€ unit_tests.py
â”‚Â Â  â”œâ”€â”€ huggingface
â”‚Â Â  â”‚Â Â  â””â”€â”€ hub
â”‚Â Â  â”œâ”€â”€ tracking
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ Dockerfile
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ model_metrics1.csv
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ model_metrics2.csv
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ models.py
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ pip_cache
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ requirements.txt
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ services.py
â”‚Â Â  â”‚Â Â  â””â”€â”€ tracking_api.py
â”‚Â Â  â””â”€â”€ u2net
â”‚Â Â      â””â”€â”€ u2net.onnx
â”œâ”€â”€ README.md
â””â”€â”€ user
    â””â”€â”€ api
        â”œâ”€â”€ Dockerfile
        â”œâ”€â”€ favicon.ico
        â”œâ”€â”€ models.py
        â”œâ”€â”€ pip_cache
        â”œâ”€â”€ requirements.txt
        â”œâ”€â”€ services.py
        â”œâ”€â”€ static
        â”œâ”€â”€ templates
        â”œâ”€â”€ test_user_api.md
        â”œâ”€â”€ test_user_api.py
        â””â”€â”€ user_api.py
```

## Attributions
- TripoSR Model: [VAST-AI-Research/TripoSR](https://github.com/VAST-AI-Research/TripoSR)
- Image Source: [Pixabay API](https://pixabay.com/service/about/api/)
